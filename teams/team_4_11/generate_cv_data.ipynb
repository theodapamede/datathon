{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edeb366-950c-4db6-8ef8-10d127d889cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2023-08-19 21:01:59.898862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-08-19 21:01:59.965924: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "168579it [50:53, 61.09it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from cxr_foundation import embeddings_data\n",
    "\n",
    "data_folder = '/shared/merged_deduplicate.csv'\n",
    "data = pd.read_csv(data_folder)\n",
    "all_embedding = []\n",
    "for index, row in tqdm(data.iterrows()):\n",
    "    filename = row['local_embeddings_file']\n",
    "    values = embeddings_data.read_tfrecord_values(filename)\n",
    "    all_embedding.append(values)\n",
    "all_embedding = np.array(all_embedding)\n",
    "np.save('all_embedding2.npy',all_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07498613-4dee-45d3-81a7-fa2648e27478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19627.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173913, 1376) (173913,) (19222, 1376) (19222,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19633.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173681, 1376) (173681,) (19454, 1376) (19454,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19568.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173675, 1376) (173675,) (19460, 1376) (19460,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19300.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173833, 1376) (173833,) (19302, 1376) (19302,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19251.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174018, 1376) (174018,) (19117, 1376) (19117,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19473.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173953, 1376) (173953,) (19182, 1376) (19182,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19365.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173893, 1376) (173893,) (19242, 1376) (19242,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19700.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173647, 1376) (173647,) (19488, 1376) (19488,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19440.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174002, 1376) (174002,) (19133, 1376) (19133,) (19221, 1376) (19221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19402.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173600, 1376) (173600,) (19535, 1376) (19535,) (19221, 1376) (19221,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "test_fold = 10\n",
    "\n",
    "all_embeddings = np.load('all_embedding.npy')\n",
    "for val_fold in range(10):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    train_race = []\n",
    "    val_race = []\n",
    "    test_race = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        if True:\n",
    "            diagnosis = row['Cardiomegaly']\n",
    "            if diagnosis != -1:\n",
    "                filename = row['local_embeddings_file']\n",
    "                race = row['race']\n",
    "                values = all_embeddings[index]\n",
    "                fold_number = row['fold']\n",
    "                if diagnosis == 1:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                if fold_number == test_fold:\n",
    "                    x_test.append(values)\n",
    "                    y_test.append(label)\n",
    "                    test_race.append(race)\n",
    "                elif fold_number == val_fold:\n",
    "                    x_val.append(values)\n",
    "                    y_val.append(label)\n",
    "                    val_race.append(race)\n",
    "                else:\n",
    "                    x_train.append(values)\n",
    "                    y_train.append(label)\n",
    "                    train_race.append(race)\n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, x_val.shape,y_val.shape,x_test.shape, y_test.shape)\n",
    "    np.save('first_exp_data/x_train_%d.npy'%val_fold,x_train)\n",
    "    np.save('first_exp_data/y_train_%d.npy'%val_fold,y_train)\n",
    "    np.save('first_exp_data/x_val_%d.npy'%val_fold,x_val)\n",
    "    np.save('first_exp_data/y_val_%d.npy'%val_fold,y_val)\n",
    "    np.save('first_exp_data/x_test.npy',x_test)\n",
    "    np.save('first_exp_data/y_test.npy',y_test)\n",
    "    with open('first_exp_data/train_race_%d.pkl'%val_fold,'wb') as f1:\n",
    "        pk.dump(train_race,f1)\n",
    "    with open('first_exp_data/val_race_%d.pkl'%val_fold,'wb') as f2:\n",
    "        pk.dump(val_race,f2)\n",
    "    with open('first_exp_data/test_race.pkl','wb') as f3:\n",
    "        pk.dump(test_race,f3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdb31cd-4a5a-4e2e-a065-f8206d0c7f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218131"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data_folder = '/shared/merged_deduplicate.csv'\n",
    "data = pd.read_csv(data_folder)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2714bd-1c8f-482b-8f1d-3605820d674f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'embeddings_file', 'subject_id',\n",
       "       'study_id', 'dicom_id', 'local_embeddings_file', 'study_id_x',\n",
       "       'subject_id_x', 'split', 'Atelectasis', 'Cardiomegaly', 'Consolidation',\n",
       "       'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
       "       'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
       "       'Pneumonia', 'Pneumothorax', 'Support Devices', 'subject_id_y',\n",
       "       'study_id_y', 'PerformedProcedureStepDescription', 'ViewPosition',\n",
       "       'Rows', 'Columns', 'StudyDate', 'StudyTime',\n",
       "       'ProcedureCodeSequence_CodeMeaning', 'ViewCodeSequence_CodeMeaning',\n",
       "       'PatientOrientationCodeSequence_CodeMeaning', 'hadm_id', 'admittime',\n",
       "       'dischtime', 'deathtime', 'admission_type', 'admit_provider_id',\n",
       "       'admission_location', 'discharge_location', 'insurance', 'language',\n",
       "       'marital_status', 'race', 'edregtime', 'edouttime',\n",
       "       'hospital_expire_flag', 'fold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1350a8c-5fb9-4d3e-94bf-1143cfc0f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WHITE', 'MISSING', 'BLACK/AFRICAN AMERICAN', 'WHITE - RUSSIAN',\n",
       "       'PORTUGUESE', 'OTHER', 'BLACK/CAPE VERDEAN', 'UNKNOWN', 'ASIAN',\n",
       "       'ASIAN - CHINESE', 'HISPANIC/LATINO - GUATEMALAN',\n",
       "       'WHITE - BRAZILIAN', 'HISPANIC/LATINO - PUERTO RICAN',\n",
       "       'HISPANIC OR LATINO', 'WHITE - OTHER EUROPEAN',\n",
       "       'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER', 'UNABLE TO OBTAIN',\n",
       "       'PATIENT DECLINED TO ANSWER', 'BLACK/CARIBBEAN ISLAND',\n",
       "       'WHITE - EASTERN EUROPEAN', 'BLACK/AFRICAN',\n",
       "       'ASIAN - SOUTH EAST ASIAN', 'HISPANIC/LATINO - CUBAN',\n",
       "       'AMERICAN INDIAN/ALASKA NATIVE', 'HISPANIC/LATINO - DOMINICAN',\n",
       "       'ASIAN - ASIAN INDIAN', 'HISPANIC/LATINO - COLUMBIAN',\n",
       "       'HISPANIC/LATINO - CENTRAL AMERICAN', 'HISPANIC/LATINO - HONDURAN',\n",
       "       'HISPANIC/LATINO - SALVADORAN', 'ASIAN - KOREAN',\n",
       "       'MULTIPLE RACE/ETHNICITY', 'SOUTH AMERICAN',\n",
       "       'HISPANIC/LATINO - MEXICAN'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00a14f18-981f-46c0-bcdb-25da20c7ec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19801.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101473, 1376) (101473,) (11012, 1376) (11012,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19596.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101218, 1376) (101218,) (11267, 1376) (11267,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19480.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101125, 1376) (101125,) (11360, 1376) (11360,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19792.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101072, 1376) (101072,) (11413, 1376) (11413,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19702.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101290, 1376) (101290,) (11195, 1376) (11195,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19719.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101448, 1376) (101448,) (11037, 1376) (11037,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19689.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101280, 1376) (101280,) (11205, 1376) (11205,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19636.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101138, 1376) (101138,) (11347, 1376) (11347,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19094.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101373, 1376) (101373,) (11112, 1376) (11112,) (2891, 1376) (2891,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18989.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100948, 1376) (100948,) (11537, 1376) (11537,) (2891, 1376) (2891,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "test_fold = 10\n",
    "\n",
    "all_embeddings = np.load('all_embedding.npy')\n",
    "for val_fold in range(10):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    train_race = []\n",
    "    val_race = []\n",
    "    test_race = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        if True:\n",
    "            diagnosis = row['Cardiomegaly']\n",
    "            if diagnosis != -1:\n",
    "                filename = row['local_embeddings_file']\n",
    "                race = row['race']\n",
    "                values = all_embeddings[index]\n",
    "                fold_number = row['fold']\n",
    "                if diagnosis == 1:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                if fold_number == test_fold:\n",
    "                    if 'BLACK' in race:\n",
    "                        x_test.append(values)\n",
    "                        y_test.append(label)\n",
    "\n",
    "                elif fold_number == val_fold:\n",
    "                    if 'WHITE' in race:\n",
    "                        x_val.append(values)\n",
    "                        y_val.append(label)\n",
    "                  \n",
    "                else:\n",
    "                    if 'WHITE' in race:\n",
    "                        x_train.append(values)\n",
    "                        y_train.append(label)\n",
    "                    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, x_val.shape,y_val.shape,x_test.shape, y_test.shape)\n",
    "    np.save('second_exp_data/x_train_%d.npy'%val_fold,x_train)\n",
    "    np.save('second_exp_data/y_train_%d.npy'%val_fold,y_train)\n",
    "    np.save('second_exp_data/x_val_%d.npy'%val_fold,x_val)\n",
    "    np.save('second_exp_data/y_val_%d.npy'%val_fold,y_val)\n",
    "    np.save('second_exp_data/x_test.npy',x_test)\n",
    "    np.save('second_exp_data/y_test.npy',y_test)\n",
    "    # with open('second_exp_data/train_race_%d.pkl'%val_fold,'wb') as f1:\n",
    "    #     pk.dump(train_race,f1)\n",
    "    # with open('second_exp_data/val_race_%d.pkl'%val_fold,'wb') as f2:\n",
    "    #     pk.dump(val_race,f2)\n",
    "    # with open('second_exp_data/test_race.pkl','wb') as f3:\n",
    "    #     pk.dump(test_race,f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb2ab69-5add-4481-942f-9ac37b8f292b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19678.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25674, 1376) (25674,) (2952, 1376) (2952,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:10, 19909.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25684, 1376) (25684,) (2942, 1376) (2942,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19565.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25844, 1376) (25844,) (2782, 1376) (2782,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19726.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26024, 1376) (26024,) (2602, 1376) (2602,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19670.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25906, 1376) (25906,) (2720, 1376) (2720,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19530.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25567, 1376) (25567,) (3059, 1376) (3059,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19744.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25883, 1376) (25883,) (2743, 1376) (2743,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 19762.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25672, 1376) (25672,) (2954, 1376) (2954,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:10, 19899.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25572, 1376) (25572,) (3054, 1376) (3054,) (10933, 1376) (10933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:10, 19921.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25808, 1376) (25808,) (2818, 1376) (2818,) (10933, 1376) (10933,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "test_fold = 10\n",
    "\n",
    "all_embeddings = np.load('all_embedding.npy')\n",
    "for val_fold in range(10):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    train_race = []\n",
    "    val_race = []\n",
    "    test_race = []\n",
    "    for index, row in tqdm(data.iterrows()):\n",
    "        if True:\n",
    "            diagnosis = row['Cardiomegaly']\n",
    "            if diagnosis != -1:\n",
    "                filename = row['local_embeddings_file']\n",
    "                race = row['race']\n",
    "                values = all_embeddings[index]\n",
    "                fold_number = row['fold']\n",
    "                if diagnosis == 1:\n",
    "                    label = 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                if fold_number == test_fold:\n",
    "                    if 'WHITE' in race:\n",
    "                        x_test.append(values)\n",
    "                        y_test.append(label)\n",
    "\n",
    "                elif fold_number == val_fold:\n",
    "                    if 'BLACK' in race:\n",
    "                        x_val.append(values)\n",
    "                        y_val.append(label)\n",
    "                  \n",
    "                else:\n",
    "                    if 'BLACK' in race:\n",
    "                        x_train.append(values)\n",
    "                        y_train.append(label)\n",
    "                    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, x_val.shape,y_val.shape,x_test.shape, y_test.shape)\n",
    "    np.save('third_exp_data/x_train_%d.npy'%val_fold,x_train)\n",
    "    np.save('third_exp_data/y_train_%d.npy'%val_fold,y_train)\n",
    "    np.save('third_exp_data/x_val_%d.npy'%val_fold,x_val)\n",
    "    np.save('third_exp_data/y_val_%d.npy'%val_fold,y_val)\n",
    "    np.save('third_exp_data/x_test.npy',x_test)\n",
    "    np.save('third_exp_data/y_test.npy',y_test)\n",
    "    # with open('second_exp_data/train_race_%d.pkl'%val_fold,'wb') as f1:\n",
    "    #     pk.dump(train_race,f1)\n",
    "    # with open('second_exp_data/val_race_%d.pkl'%val_fold,'wb') as f2:\n",
    "    #     pk.dump(val_race,f2)\n",
    "    # with open('second_exp_data/test_race.pkl','wb') as f3:\n",
    "    #     pk.dump(test_race,f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fbb11f1-f15f-4619-ba11-699a94b7e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "data_folder = '/shared/merged_deduplicate.csv'\n",
    "data = pd.read_csv(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebf36f7-c2e5-401c-b640-35ab2e589a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_file = '/shared/mimic4_gender.csv'\n",
    "gender_data = pd.read_csv(gender_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c55b9123-5118-4dcc-94dd-e0877f0ce7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10137012</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10736768</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10782862</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11024137</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11516471</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id gender\n",
       "0    10137012      F\n",
       "1    10736768      F\n",
       "2    10782862      F\n",
       "3    11024137      F\n",
       "4    11516471      F"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ba941b-92ab-4fb0-9c7b-7878b99c0cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/shared/merged_deduplicate.csv'\n",
    "data = pd.read_csv(data_folder)\n",
    "merged_gender = data.merge(gender_data,how='left', on='subject_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7871d98e-8e30-4e7f-9dcf-b10f7668cecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218131"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb448a5e-d1d2-4ba0-b8ee-c91740a902ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18645.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83895, 1376) (83895,) (9542, 1376) (9542,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18589.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83941, 1376) (83941,) (9496, 1376) (9496,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18627.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84217, 1376) (84217,) (9220, 1376) (9220,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18619.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84213, 1376) (84213,) (9224, 1376) (9224,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18758.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83769, 1376) (83769,) (9668, 1376) (9668,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18512.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84364, 1376) (84364,) (9073, 1376) (9073,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18696.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84246, 1376) (84246,) (9191, 1376) (9191,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18729.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83882, 1376) (83882,) (9555, 1376) (9555,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18773.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84245, 1376) (84245,) (9192, 1376) (9192,) (9831, 1376) (9831,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "218131it [00:11, 18346.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84161, 1376) (84161,) (9276, 1376) (9276,) (9831, 1376) (9831,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pk\n",
    "test_fold = 10\n",
    "\n",
    "\n",
    "for val_fold in range(10):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_val = []\n",
    "    y_val = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    train_race = []\n",
    "    val_race = []\n",
    "    test_race = []\n",
    "    for index, row in tqdm(merged_gender.iterrows()):\n",
    "        if True:\n",
    "            diagnosis = row['Cardiomegaly']\n",
    "            \n",
    "            gender = row['gender']\n",
    "            if diagnosis != -1:\n",
    "                if gender in ['M','F']:\n",
    "                    filename = row['local_embeddings_file']\n",
    "                    race = row['race']\n",
    "                    values = all_embeddings[index]\n",
    "                    fold_number = row['fold']\n",
    "                    if diagnosis == 1:\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                    if fold_number == test_fold:\n",
    "                        if 'M' in gender:\n",
    "                            x_test.append(values)\n",
    "                            y_test.append(label)\n",
    "    \n",
    "                    elif fold_number == val_fold:\n",
    "                        if 'F' in gender:\n",
    "                            x_val.append(values)\n",
    "                            y_val.append(label)\n",
    "                      \n",
    "                    else:\n",
    "                        if 'F' in gender:\n",
    "                            x_train.append(values)\n",
    "                            y_train.append(label)\n",
    "                    \n",
    "    \n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    x_test = np.array(x_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    x_val = np.array(x_val)\n",
    "    y_val = np.array(y_val)\n",
    "    \n",
    "    print(x_train.shape, y_train.shape, x_val.shape,y_val.shape,x_test.shape, y_test.shape)\n",
    "    np.save('gender_exp_data2/x_train_%d.npy'%val_fold,x_train)\n",
    "    np.save('gender_exp_data2/y_train_%d.npy'%val_fold,y_train)\n",
    "    np.save('gender_exp_data2/x_val_%d.npy'%val_fold,x_val)\n",
    "    np.save('gender_exp_data2/y_val_%d.npy'%val_fold,y_val)\n",
    "    np.save('gender_exp_data2/x_test.npy',x_test)\n",
    "    np.save('gender_exp_data2/y_test.npy',y_test)\n",
    "    # with open('second_exp_data/train_race_%d.pkl'%val_fold,'wb') as f1:\n",
    "    #     pk.dump(train_race,f1)\n",
    "    # with open('second_exp_data/val_race_%d.pkl'%val_fold,'wb') as f2:\n",
    "    #     pk.dump(val_race,f2)\n",
    "    # with open('second_exp_data/test_race.pkl','wb') as f3:\n",
    "    #     pk.dump(test_race,f3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0.1",
   "language": "python",
   "name": "torch_2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
