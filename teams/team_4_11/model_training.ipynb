{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebfb810c-3edc-406b-967a-16a30ccfb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class WhiteningLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(WhiteningLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_centered = x - torch.mean(x, dim=1, keepdim=True)\n",
    "        cov_matrix = torch.mm(x_centered, x_centered.t()) / (self.input_size - 1)\n",
    "        u, s, _ = torch.svd(cov_matrix)\n",
    "        whitened = torch.mm(u / torch.sqrt(s + 1e-6), x_centered)\n",
    "        return whitened\n",
    "\n",
    "class MLPWithWhitening(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, use_whitening=True):\n",
    "        super(MLPWithWhitening, self).__init__()\n",
    "        self.use_whitening = use_whitening\n",
    "        \n",
    "        if self.use_whitening:\n",
    "            self.whitening = WhiteningLayer(input_size)\n",
    "            \n",
    "        self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "        self.hidden2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_whitening:\n",
    "            x = self.whitening(x)\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd98297e-eb2e-493b-b601-84cebcb0cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, data_path, label_path):\n",
    "        self.data = np.load(data_path)\n",
    "        self.labels = np.load(label_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a577c56-d20e-425b-8151-839b5006fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input size and number of samples\n",
    "input_size = 1376\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "# Define your training settings\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "use_whitening = False  # Change this based on your preference\n",
    "\n",
    "# Create an instance of the MLPWithWhitening model\n",
    "model = MLPWithWhitening(input_size, hidden_size, output_size, use_whitening)\n",
    "model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3044728c-99eb-434e-a2ba-ebcab7a6fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.4090\n",
      "Epoch [1/10], Val Loss: 0.3877, Val Accuracy: 0.8155\n",
      "Epoch [2/10], Train Loss: 0.3927\n",
      "Epoch [2/10], Val Loss: 0.3884, Val Accuracy: 0.8145\n",
      "Epoch [3/10], Train Loss: 0.3905\n",
      "Epoch [3/10], Val Loss: 0.3918, Val Accuracy: 0.8158\n",
      "Epoch [4/10], Train Loss: 0.3893\n",
      "Epoch [4/10], Val Loss: 0.3874, Val Accuracy: 0.8167\n",
      "Epoch [5/10], Train Loss: 0.3884\n",
      "Epoch [5/10], Val Loss: 0.3866, Val Accuracy: 0.8133\n",
      "Epoch [6/10], Train Loss: 0.3873\n",
      "Epoch [6/10], Val Loss: 0.3946, Val Accuracy: 0.8165\n",
      "Epoch [7/10], Train Loss: 0.3865\n",
      "Epoch [7/10], Val Loss: 0.3846, Val Accuracy: 0.8168\n",
      "Epoch [8/10], Train Loss: 0.3854\n",
      "Epoch [8/10], Val Loss: 0.3878, Val Accuracy: 0.8168\n",
      "Epoch [9/10], Train Loss: 0.3849\n",
      "Epoch [9/10], Val Loss: 0.3872, Val Accuracy: 0.8159\n",
      "Epoch [10/10], Train Loss: 0.3846\n",
      "Epoch [10/10], Val Loss: 0.3843, Val Accuracy: 0.8166\n",
      "Epoch [1/10], Train Loss: 0.4072\n",
      "Epoch [1/10], Val Loss: 0.3947, Val Accuracy: 0.8112\n",
      "Epoch [2/10], Train Loss: 0.3916\n",
      "Epoch [2/10], Val Loss: 0.3936, Val Accuracy: 0.8123\n",
      "Epoch [3/10], Train Loss: 0.3900\n",
      "Epoch [3/10], Val Loss: 0.3928, Val Accuracy: 0.8117\n",
      "Epoch [4/10], Train Loss: 0.3887\n",
      "Epoch [4/10], Val Loss: 0.3923, Val Accuracy: 0.8128\n",
      "Epoch [5/10], Train Loss: 0.3880\n",
      "Epoch [5/10], Val Loss: 0.3953, Val Accuracy: 0.8125\n",
      "Epoch [6/10], Train Loss: 0.3870\n",
      "Epoch [6/10], Val Loss: 0.3933, Val Accuracy: 0.8102\n",
      "Epoch [7/10], Train Loss: 0.3863\n",
      "Epoch [7/10], Val Loss: 0.3905, Val Accuracy: 0.8130\n",
      "Epoch [8/10], Train Loss: 0.3854\n",
      "Epoch [8/10], Val Loss: 0.3900, Val Accuracy: 0.8129\n",
      "Epoch [9/10], Train Loss: 0.3849\n",
      "Epoch [9/10], Val Loss: 0.3911, Val Accuracy: 0.8123\n",
      "Epoch [10/10], Train Loss: 0.3841\n",
      "Epoch [10/10], Val Loss: 0.3999, Val Accuracy: 0.8135\n",
      "Epoch [1/10], Train Loss: 20.2243\n",
      "Epoch [1/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [2/10], Train Loss: 20.2857\n",
      "Epoch [2/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [3/10], Train Loss: 20.2845\n",
      "Epoch [3/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [4/10], Train Loss: 20.2874\n",
      "Epoch [4/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [5/10], Train Loss: 20.2860\n",
      "Epoch [5/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [6/10], Train Loss: 20.2854\n",
      "Epoch [6/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [7/10], Train Loss: 20.2868\n",
      "Epoch [7/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [8/10], Train Loss: 20.2857\n",
      "Epoch [8/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [9/10], Train Loss: 20.2857\n",
      "Epoch [9/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [10/10], Train Loss: 20.2862\n",
      "Epoch [10/10], Val Loss: 20.1895, Val Accuracy: 0.7983\n",
      "Epoch [1/10], Train Loss: 0.4081\n",
      "Epoch [1/10], Val Loss: 0.3990, Val Accuracy: 0.8087\n",
      "Epoch [2/10], Train Loss: 0.3927\n",
      "Epoch [2/10], Val Loss: 0.3984, Val Accuracy: 0.8038\n",
      "Epoch [3/10], Train Loss: 0.3900\n",
      "Epoch [3/10], Val Loss: 0.3974, Val Accuracy: 0.8040\n",
      "Epoch [4/10], Train Loss: 0.3889\n",
      "Epoch [4/10], Val Loss: 0.3949, Val Accuracy: 0.8097\n",
      "Epoch [5/10], Train Loss: 0.3872\n",
      "Epoch [5/10], Val Loss: 0.3962, Val Accuracy: 0.8087\n",
      "Epoch [6/10], Train Loss: 0.3864\n",
      "Epoch [6/10], Val Loss: 0.3910, Val Accuracy: 0.8101\n",
      "Epoch [7/10], Train Loss: 0.3854\n",
      "Epoch [7/10], Val Loss: 0.3909, Val Accuracy: 0.8119\n",
      "Epoch [8/10], Train Loss: 0.3846\n",
      "Epoch [8/10], Val Loss: 0.3946, Val Accuracy: 0.8104\n",
      "Epoch [9/10], Train Loss: 0.3840\n",
      "Epoch [9/10], Val Loss: 0.3881, Val Accuracy: 0.8124\n",
      "Epoch [10/10], Train Loss: 0.3830\n",
      "Epoch [10/10], Val Loss: 0.3898, Val Accuracy: 0.8112\n",
      "Epoch [1/10], Train Loss: 0.4053\n",
      "Epoch [1/10], Val Loss: 0.3932, Val Accuracy: 0.8132\n",
      "Epoch [2/10], Train Loss: 0.3929\n",
      "Epoch [2/10], Val Loss: 0.3934, Val Accuracy: 0.8127\n",
      "Epoch [3/10], Train Loss: 0.3902\n",
      "Epoch [3/10], Val Loss: 0.3907, Val Accuracy: 0.8134\n",
      "Epoch [4/10], Train Loss: 0.3884\n",
      "Epoch [4/10], Val Loss: 0.3904, Val Accuracy: 0.8138\n",
      "Epoch [5/10], Train Loss: 0.3876\n",
      "Epoch [5/10], Val Loss: 0.3882, Val Accuracy: 0.8146\n",
      "Epoch [6/10], Train Loss: 0.3867\n",
      "Epoch [6/10], Val Loss: 0.3941, Val Accuracy: 0.8099\n",
      "Epoch [7/10], Train Loss: 0.3862\n",
      "Epoch [7/10], Val Loss: 0.3861, Val Accuracy: 0.8175\n",
      "Epoch [8/10], Train Loss: 0.3850\n",
      "Epoch [8/10], Val Loss: 0.3975, Val Accuracy: 0.8132\n",
      "Epoch [9/10], Train Loss: 0.3848\n",
      "Epoch [9/10], Val Loss: 0.3877, Val Accuracy: 0.8152\n",
      "Epoch [10/10], Train Loss: 0.3841\n",
      "Epoch [10/10], Val Loss: 0.3861, Val Accuracy: 0.8168\n",
      "Epoch [1/10], Train Loss: 0.5988\n",
      "Epoch [1/10], Val Loss: 0.8161, Val Accuracy: 0.7919\n",
      "Epoch [2/10], Train Loss: 0.4166\n",
      "Epoch [2/10], Val Loss: 0.5297, Val Accuracy: 0.7935\n",
      "Epoch [3/10], Train Loss: 0.4123\n",
      "Epoch [3/10], Val Loss: 0.3955, Val Accuracy: 0.8095\n",
      "Epoch [4/10], Train Loss: 0.4068\n",
      "Epoch [4/10], Val Loss: 0.3960, Val Accuracy: 0.8074\n",
      "Epoch [5/10], Train Loss: 0.3975\n",
      "Epoch [5/10], Val Loss: 0.5072, Val Accuracy: 0.7123\n",
      "Epoch [6/10], Train Loss: 0.3935\n",
      "Epoch [6/10], Val Loss: 0.4356, Val Accuracy: 0.7713\n",
      "Epoch [7/10], Train Loss: 0.3905\n",
      "Epoch [7/10], Val Loss: 0.3942, Val Accuracy: 0.8099\n",
      "Epoch [8/10], Train Loss: 0.3885\n",
      "Epoch [8/10], Val Loss: 0.3949, Val Accuracy: 0.8079\n",
      "Epoch [9/10], Train Loss: 0.3885\n",
      "Epoch [9/10], Val Loss: 0.4046, Val Accuracy: 0.8040\n",
      "Epoch [10/10], Train Loss: 0.3867\n",
      "Epoch [10/10], Val Loss: 0.3924, Val Accuracy: 0.8094\n",
      "Epoch [1/10], Train Loss: 0.4067\n",
      "Epoch [1/10], Val Loss: 0.3923, Val Accuracy: 0.8141\n",
      "Epoch [2/10], Train Loss: 0.3924\n",
      "Epoch [2/10], Val Loss: 0.3909, Val Accuracy: 0.8161\n",
      "Epoch [3/10], Train Loss: 0.3901\n",
      "Epoch [3/10], Val Loss: 0.3907, Val Accuracy: 0.8154\n",
      "Epoch [4/10], Train Loss: 0.3891\n",
      "Epoch [4/10], Val Loss: 0.3904, Val Accuracy: 0.8177\n",
      "Epoch [5/10], Train Loss: 0.3876\n",
      "Epoch [5/10], Val Loss: 0.4004, Val Accuracy: 0.8131\n",
      "Epoch [6/10], Train Loss: 0.3860\n",
      "Epoch [6/10], Val Loss: 0.3891, Val Accuracy: 0.8177\n",
      "Epoch [7/10], Train Loss: 0.3850\n",
      "Epoch [7/10], Val Loss: 0.3943, Val Accuracy: 0.8175\n",
      "Epoch [8/10], Train Loss: 0.3846\n",
      "Epoch [8/10], Val Loss: 0.3894, Val Accuracy: 0.8161\n",
      "Epoch [9/10], Train Loss: 0.3838\n",
      "Epoch [9/10], Val Loss: 0.3903, Val Accuracy: 0.8169\n",
      "Epoch [10/10], Train Loss: 0.3831\n",
      "Epoch [10/10], Val Loss: 0.3897, Val Accuracy: 0.8146\n",
      "Epoch [1/10], Train Loss: 19.7127\n",
      "Epoch [1/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [2/10], Train Loss: 19.8482\n",
      "Epoch [2/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [3/10], Train Loss: 19.8462\n",
      "Epoch [3/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [4/10], Train Loss: 19.8426\n",
      "Epoch [4/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [5/10], Train Loss: 19.8444\n",
      "Epoch [5/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [6/10], Train Loss: 19.8426\n",
      "Epoch [6/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [7/10], Train Loss: 19.8426\n",
      "Epoch [7/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [8/10], Train Loss: 19.8426\n",
      "Epoch [8/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [9/10], Train Loss: 19.8499\n",
      "Epoch [9/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [10/10], Train Loss: 19.8481\n",
      "Epoch [10/10], Val Loss: 19.6532, Val Accuracy: 0.7989\n",
      "Epoch [1/10], Train Loss: 0.4032\n",
      "Epoch [1/10], Val Loss: 0.3927, Val Accuracy: 0.8126\n",
      "Epoch [2/10], Train Loss: 0.3925\n",
      "Epoch [2/10], Val Loss: 0.3949, Val Accuracy: 0.8134\n",
      "Epoch [3/10], Train Loss: 0.3906\n",
      "Epoch [3/10], Val Loss: 0.3910, Val Accuracy: 0.8160\n",
      "Epoch [4/10], Train Loss: 0.3888\n",
      "Epoch [4/10], Val Loss: 0.3934, Val Accuracy: 0.8142\n",
      "Epoch [5/10], Train Loss: 0.3875\n",
      "Epoch [5/10], Val Loss: 0.3947, Val Accuracy: 0.8145\n",
      "Epoch [6/10], Train Loss: 0.3868\n",
      "Epoch [6/10], Val Loss: 0.3880, Val Accuracy: 0.8158\n",
      "Epoch [7/10], Train Loss: 0.3852\n",
      "Epoch [7/10], Val Loss: 0.3907, Val Accuracy: 0.8140\n",
      "Epoch [8/10], Train Loss: 0.3845\n",
      "Epoch [8/10], Val Loss: 0.3875, Val Accuracy: 0.8169\n",
      "Epoch [9/10], Train Loss: 0.3836\n",
      "Epoch [9/10], Val Loss: 0.3882, Val Accuracy: 0.8168\n",
      "Epoch [10/10], Train Loss: 0.3834\n",
      "Epoch [10/10], Val Loss: 0.3871, Val Accuracy: 0.8168\n",
      "Epoch [1/10], Train Loss: 0.4119\n",
      "Epoch [1/10], Val Loss: 0.3917, Val Accuracy: 0.8144\n",
      "Epoch [2/10], Train Loss: 0.3926\n",
      "Epoch [2/10], Val Loss: 0.3899, Val Accuracy: 0.8156\n",
      "Epoch [3/10], Train Loss: 0.3907\n",
      "Epoch [3/10], Val Loss: 0.3894, Val Accuracy: 0.8150\n",
      "Epoch [4/10], Train Loss: 0.3895\n",
      "Epoch [4/10], Val Loss: 0.3890, Val Accuracy: 0.8164\n",
      "Epoch [5/10], Train Loss: 0.3889\n",
      "Epoch [5/10], Val Loss: 0.3882, Val Accuracy: 0.8166\n",
      "Epoch [6/10], Train Loss: 0.3881\n",
      "Epoch [6/10], Val Loss: 0.3864, Val Accuracy: 0.8170\n",
      "Epoch [7/10], Train Loss: 0.3872\n",
      "Epoch [7/10], Val Loss: 0.3880, Val Accuracy: 0.8150\n",
      "Epoch [8/10], Train Loss: 0.3865\n",
      "Epoch [8/10], Val Loss: 0.3860, Val Accuracy: 0.8179\n",
      "Epoch [9/10], Train Loss: 0.3861\n",
      "Epoch [9/10], Val Loss: 0.3862, Val Accuracy: 0.8158\n",
      "Epoch [10/10], Train Loss: 0.3855\n",
      "Epoch [10/10], Val Loss: 0.3901, Val Accuracy: 0.8162\n"
     ]
    }
   ],
   "source": [
    "for fold in range(10):\n",
    "        # Create instances of the dataset class\n",
    "    train_dataset = DummyDataset('first_exp_data/x_train_%d.npy'%fold, 'first_exp_data/y_train_%d.npy'%fold)\n",
    "    val_dataset = DummyDataset('first_exp_data/x_val_%d.npy'%fold, 'first_exp_data/y_val_%d.npy'%fold)\n",
    "\n",
    "    # Define batch size for the data loaders\n",
    "    \n",
    "    # Create data loaders for the datasets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.apply(init_weights)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        for batch_data, batch_labels in train_dataloader:\n",
    "            batch_data = batch_data.to(device).float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1]\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        average_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {average_loss:.4f}\")\n",
    "    \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_dataloader:\n",
    "                batch_data = batch_data.to(device).float()\n",
    "                batch_labels = batch_labels.float()\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                outputs = F.softmax(outputs, dim=1)\n",
    "                outputs = outputs[:, 1]\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                predicted = outputs > 0.5\n",
    "                total += batch_labels.size(0)\n",
    "                correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_accuracy = correct / total\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "        # save model with least validation loss\n",
    "        if epoch == 0:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_first_exp_%d.pth'%fold)\n",
    "        else:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model_first_exp_%d.pth'%fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a073fc-33d4-438f-ac8a-d6cbd0995a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.826747508401413\n",
      "AUROC: 0.8260121864968887\n",
      "AUROC: 0.8262101020803064\n",
      "AUROC: 0.8253067243302108\n",
      "AUROC: 0.8238726207719507\n",
      "AUROC: 0.8267878155994993\n",
      "AUROC: 0.8274610992438383\n",
      "AUROC: 0.8248889552208751\n",
      "0.8259108765181228 0.0010917746275846277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, recall_score, precision_score, \\\n",
    "    f1_score,roc_curve,auc\n",
    "\n",
    "auc_list = []\n",
    "for fold in range(10):\n",
    "    model.load_state_dict(torch.load('best_model_first_exp_%d.pth'%fold))\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prediction = []\n",
    "    test_dataset = DummyDataset('first_exp_data/x_test.npy', 'first_exp_data/y_test.npy')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_dataloader:\n",
    "            batch_data = batch_data.to(device).float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1]\n",
    "            prediction.append(outputs.detach().cpu().numpy())\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted = outputs > 0.5\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_accuracy = correct / total\n",
    "    y_true = np.load('first_exp_data/y_test.npy')\n",
    "    prediction = np.concatenate(prediction)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true,prediction)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    if auc_value>0.6:\n",
    "        auc_list.append(auc_value)\n",
    "        print('AUROC:',auc_value)\n",
    "\n",
    "auc_list = np.array(auc_list)\n",
    "\n",
    "print(auc_list.mean(),auc_list.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29579dd4-9cdc-4401-93d5-a5ac070a1f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHITE - AUROC: 0.8153681717102067\n",
      "BLACK - AUROC: 0.8287208017737059\n",
      "ASIAN - AUROC: 0.837809172287981\n",
      "HISPANIC - AUROC: 0.8459180216802167\n",
      "OTHER - AUROC: 0.8495176968319252\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8153270034054667\n",
      "BLACK - AUROC: 0.8265105455123318\n",
      "ASIAN - AUROC: 0.8302999448474822\n",
      "HISPANIC - AUROC: 0.842970867208672\n",
      "OTHER - AUROC: 0.8496721751866084\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8153834401044973\n",
      "BLACK - AUROC: 0.827591225595883\n",
      "ASIAN - AUROC: 0.840418310635951\n",
      "HISPANIC - AUROC: 0.8447239159891599\n",
      "OTHER - AUROC: 0.8476730168897845\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8144182265278411\n",
      "BLACK - AUROC: 0.8275679235509565\n",
      "ASIAN - AUROC: 0.8304908574095287\n",
      "HISPANIC - AUROC: 0.8407435636856367\n",
      "OTHER - AUROC: 0.8483960299515861\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8127711869209091\n",
      "BLACK - AUROC: 0.824559285750303\n",
      "ASIAN - AUROC: 0.8315302702473378\n",
      "HISPANIC - AUROC: 0.8407520325203252\n",
      "OTHER - AUROC: 0.8478605653907781\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8160321675350694\n",
      "BLACK - AUROC: 0.8275373634920369\n",
      "ASIAN - AUROC: 0.836175809257138\n",
      "HISPANIC - AUROC: 0.8445121951219512\n",
      "OTHER - AUROC: 0.8499276760430348\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8163792673240684\n",
      "BLACK - AUROC: 0.8292151107267335\n",
      "ASIAN - AUROC: 0.8372788596156294\n",
      "HISPANIC - AUROC: 0.8444359756097563\n",
      "OTHER - AUROC: 0.8501079762927823\n",
      "------------------------------------------------------------\n",
      "WHITE - AUROC: 0.8141771601012059\n",
      "BLACK - AUROC: 0.8245229956803355\n",
      "ASIAN - AUROC: 0.8276059564719358\n",
      "HISPANIC - AUROC: 0.8415396341463415\n",
      "OTHER - AUROC: 0.8497439781755156\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "with open('first_exp_data/test_race.pkl', 'rb') as f:\n",
    "    race_test = pk.load(f)\n",
    "\n",
    "race_index = []\n",
    "for race in race_test:\n",
    "    if 'WHITE' in race:\n",
    "        race_index.append(0)\n",
    "    elif 'BLACK' in race:\n",
    "        race_index.append(1)\n",
    "    elif 'ASIAN' in race:\n",
    "        race_index.append(2)\n",
    "    elif 'HISPANIC' in race:\n",
    "        race_index.append(3)\n",
    "    else:\n",
    "        race_index.append(4)\n",
    "\n",
    "race_dict = {0: 'WHITE', 1: 'BLACK', 2: 'ASIAN', 3: 'HISPANIC', 4: 'OTHER'}\n",
    "race_index = np.array(race_index)\n",
    "auc_list = []\n",
    "for fold in range(10):\n",
    "    model.load_state_dict(torch.load('best_model_first_exp_%d.pth' % fold))\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prediction = []\n",
    "    test_dataset = DummyDataset('first_exp_data/x_test.npy', 'first_exp_data/y_test.npy')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_dataloader:\n",
    "            batch_data = batch_data.to(device).float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1]\n",
    "            prediction.append(outputs.detach().cpu().numpy())\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted = outputs > 0.5\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    y_true = np.load('first_exp_data/y_test.npy')\n",
    "    prediction = np.concatenate(prediction)\n",
    "    for rc in range(0,5):\n",
    "        rc_index = race_index == rc\n",
    "        rc_y_true = y_true[rc_index]\n",
    "        rc_prediction = prediction[rc_index]\n",
    "        fpr, tpr, thresholds = roc_curve(rc_y_true, rc_prediction)\n",
    "        auc_value = auc(fpr, tpr)\n",
    "        if auc_value > 0.6:\n",
    "            auc_list.append(auc_value)\n",
    "            print('%s - AUROC:'%race_dict[rc], auc_value)\n",
    "    print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efb365cb-5b0e-4f4e-8e6f-72fb2d41d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.8351782202132432\n",
      "AUROC: 0.8343578279053973\n",
      "AUROC: 0.8335517462439093\n",
      "AUROC: 0.8339586189863051\n",
      "AUROC: 0.8325097931675745\n",
      "AUROC: 0.8352575825540872\n",
      "AUROC: 0.8354103505200321\n",
      "AUROC: 0.8333116252208796\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "auc_list = []\n",
    "for fold in range(10):\n",
    "    model.load_state_dict(torch.load('best_model_first_exp_%d.pth' % fold))\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prediction = []\n",
    "    test_dataset = DummyDataset('gender_exp_data/x_test.npy', 'gender_exp_data/y_test.npy')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_dataloader:\n",
    "            batch_data = batch_data.to(device).float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1]\n",
    "            prediction.append(outputs.detach().cpu().numpy())\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted = outputs > 0.5\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    y_true = np.load('gender_exp_data/y_test.npy')\n",
    "    prediction = np.concatenate(prediction)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, prediction)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    if auc_value > 0.6:\n",
    "        auc_list.append(auc_value)\n",
    "        print('AUROC:', auc_value)\n",
    "print('------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc6ba55b-b25f-4a36-aaeb-4ec71fcb0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC: 0.8179324188647871\n",
      "AUROC: 0.8174506368216287\n",
      "AUROC: 0.8188275608920158\n",
      "AUROC: 0.8168520342229977\n",
      "AUROC: 0.8153442172082431\n",
      "AUROC: 0.8182527629886176\n",
      "AUROC: 0.8192628843241742\n",
      "AUROC: 0.8164466310569468\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pickle as pk\n",
    "\n",
    "auc_list = []\n",
    "for fold in range(10):\n",
    "    model.load_state_dict(torch.load('best_model_first_exp_%d.pth' % fold))\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    prediction = []\n",
    "    test_dataset = DummyDataset('gender_exp_data/x_test_male.npy', 'gender_exp_data/y_test_male.npy')\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_dataloader:\n",
    "            batch_data = batch_data.to(device).float()\n",
    "            batch_labels = batch_labels.float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = outputs[:, 1]\n",
    "            prediction.append(outputs.detach().cpu().numpy())\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted = outputs > 0.5\n",
    "            total += batch_labels.size(0)\n",
    "            correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_accuracy = correct / total\n",
    "\n",
    "    y_true = np.load('gender_exp_data/y_test_male.npy')\n",
    "    prediction = np.concatenate(prediction)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, prediction)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    if auc_value > 0.6:\n",
    "        auc_list.append(auc_value)\n",
    "        print('AUROC:', auc_value)\n",
    "print('------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_2.0.1",
   "language": "python",
   "name": "torch_2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
